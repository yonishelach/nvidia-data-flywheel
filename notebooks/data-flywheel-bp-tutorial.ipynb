{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "\n# Discover Cost-Efficient AI Customer Service Agents with NVIDIA Data Flywheel Blueprint\n[![ Click here to deploy.](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-2wggjBvDlVp4pLQD8ytZySh5m8W)\n\nIn this notebook, you will learn how to use the Data Flywheel Blueprint to continuously discover and promote more cost-efficient agents for an [AI virtual customer service assistant](https://build.nvidia.com/nvidia/ai-virtual-assistant-for-customer-service).\n\n### Data Flywheel Blueprint\n\n![Data Flywheel Blueprint](https://raw.githubusercontent.com/NVIDIA-AI-Blueprints/data-flywheel/update-launchable/docs/images/data-flywheel-blueprint.png)\n\n\n### AI Virtual Assistant for Customer Service\n\nThe primary customer service agent in the AI Virtual Assistant uses tool calling to route user queries to specialized assistants, including: \n\n- Product Q&A\n- Order status verification\n- Returns processing\n- Small talk and casual engagement\n\nThese interactions generate logs and tool-calling data that you can use as both evaluation benchmarks and training data. In this tutorial, you'll use this information to drive the flywheel process, fine-tuning smaller LLMs (such as `meta/llama-3.2-1B-instruct`, `meta/llama-3.2-3B-instruct`, `meta/llama-3.1-8B-instruct`) to match accuracy of the currently deployed model (`meta/llama-3.3-70B-instruct`).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Interfacing with the Blueprint\n\nThe following diagram illustrates how admin tools and applications interact with the Data Flywheel Blueprint, which orchestrates logging, processing, and model management to enable continuous optimization.\n\n![Arch](https://raw.githubusercontent.com/NVIDIA-AI-Blueprints/data-flywheel/main/notebooks/arch.png)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Contents \n\n0. [Data Flywheel Setup](#0)\n1. [Load Sample Data](#1)\n2. [Create a Flywheel Job](#2)\n3. [Monitor Job Status](#3)\n4. [Optional: Show Continuous Improvement](#4)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n<a id=\"0\"></a>\n## Data Flywheel Setup",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Step 1**: Set NGC API key following the instructions at [Generating NGC API Keys](https://docs.nvidia.com/ngc/gpu-cloud/ngc-private-registry-user-guide/index.html#generating-api-key).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import os\nfrom getpass import getpass\n\nos.environ['NGC_API_KEY'] = getpass(\"Enter your NGC API Key\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "**Step 2**: Set your docker registry credentials to allow MLRun CE installation. For more information see [MLRun CE Installation](https://docs.mlrun.org/en/latest/install/kubernetes.html#installing-the-chart).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import os\nos.environ['NGC_API_KEY'] = '<your_ngc_api_key>'\nos.environ['DOCKER_SERVER'] = '<your_docker_server>'\nos.environ['DOCKER_USERNAME'] = '<your_docker_username>'\nos.environ['DOCKER_PASSWORD'] = '<your_docker_password>'\nos.environ['DOCKER_REGISTRY_URL'] = '<your_docker_registry_url>'",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "**Step 3**: Clone the data flywheel repo and fetch data files.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "%%bash\ngit clone https://github.com/NVIDIA-AI-Blueprints/data-flywheel.git\ncd data-flywheel\nsudo apt-get update && sudo apt-get install -y git-lfs\ngit lfs install\ngit-lfs pull",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "**Step 4**: Set up paths and install python dependencies for notebook.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import sys\nfrom pathlib import Path\n\nnotebook_dir = Path.cwd()\nproject_root = notebook_dir / \"data-flywheel\"\ndata_dir = project_root / \"data\"\nsys.path.insert(0, str(project_root))\nos.chdir(project_root)\nprint(f\"Working directory changed to: {Path.cwd()}\")\n\nuser_site = Path.home() / \".local\" / \"lib\" / f\"python{sys.version_info.major}.{sys.version_info.minor}\" / \"site-packages\"\nif str(user_site) not in sys.path:\n    sys.path.append(str(user_site))\n    print(f\"Added user site-packages to sys.path: {user_site}\")\n\n%pip install --user elasticsearch==8.17.2 pydantic-settings>=2.9.1 pandas>=2.2.3 matplotlib==3.10.3",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "**Step 5**: Update `config/config.yaml` to use remote LLM as judge. By default, the Data Flywheel Blueprint deploys `LLama-3.3-70B-instruct` locally for LLM as a judge, which requires 4 GPUs. But for the launchable, we will choose the remote LLM judge and use the `LLama-3.3-70B-instruct` NIM hosted on [build.nvidia.com](https://build.nvidia.com/meta/llama-3_3-70b-instruct).\n\nBy default, only `Llama-3.2-1b-instruct` will be used in the flywheel but you can uncomment other models in the yaml file to include in the flywheel run. You can also change other config settings such as data split and training hyperparameters as desired.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import re\n\nconfig_path = project_root / \"config\" / \"config.yaml\"\nwith open(config_path, \"r\") as f:\n    original_yaml = f.read()\n\nllm_judge_config_block = \"\"\"llm_judge_config:\n  type: \"remote\"\n  url: \"https://integrate.api.nvidia.com/v1/chat/completions\"\n  model_id: \"meta/llama-3.3-70b-instruct\"\n  api_key_env: \"NGC_API_KEY\"\n\"\"\"\nupdated_yaml = re.sub(\n    r\"llm_judge_config:.*?(?=\\n\\w|\\Z)\",  # stops at next top-level key\n    llm_judge_config_block,\n    original_yaml,\n    flags=re.DOTALL\n)\n\nwith open(config_path, \"w\") as f:\n    f.write(updated_yaml)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "**Step 6**: Start data flywheel service, which involves first deploying the Nemo Microservices and then bring up the data flywheel service via docker compose. This step may take about 15 minutes.\n\n> **Note:** The `deploy-nmp.sh` script automates the deployment of NeMo Microservices. For manual setup or advanced configuration, please consult the [NeMo Microservices documentation](https://docs.nvidia.com/nemo/microservices/latest/get-started/platform-prereq.html#beginner-tutorial-prerequisites).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "%%bash\nset -e\n\nlog() {\n  echo -e \"\\033[1;32m[INFO]\\033[0m $1\"\n}\n\necho \"$NGC_API_KEY\" | docker login nvcr.io -u '$oauthtoken' --password-stdin\nchmod +x scripts/deploy-nmp.sh scripts/run.sh scripts/mlrun.sh\n\nlog \"Starting Nemo Microservices Platform (NMP) deployment...\"\n./scripts/deploy-nmp.sh >> flywheel_deploy.log 2>&1\nlog \"NMP deployed successfully!\"\n\nlog \"Starting data flywheel service...\"\n./scripts/run.sh >> flywheel_deploy.log 2>&1\nlog \"Data flywheel service started successfully!\"\n\nlog \"Starting mlrun services...\"\n./scripts/mlrun.sh >> flywheel_deploy.log 2>&1\nlog \"MLRun services deployed successfully!\"",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n<a id=\"1\"></a>\n## Step 1: Load Sample Data",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "First, we need to import required libraries and configure pandas display options for better readability in notebook outputs.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import sys\nfrom pathlib import Path\nimport requests\nimport time\nfrom datetime import datetime\nimport json\nimport pandas as pd\nfrom IPython.display import display, clear_output\n\npd.set_option('display.max_columns', None)  # Show all columns\npd.set_option('display.width', None)        # Width of the display in characters\npd.set_option('display.max_colwidth', None)  # Show full content of each cell",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Use the provided sample dataset from AI Virtual Assistant (`aiva`) (`data/aiva_primary_assistant_dataset.jsonl`) to simulate real user logs captured while an agentic customer service agent application is running. Each data point has the following schema:\n\n| Field        | Type               | Description                                                         |\n|--------------|--------------------|---------------------------------------------------------------------|\n| `timestamp`  | `int` (epoch secs) | Time the request was issued                                         |\n| `workload_id`| `str`              | Stable identifier for the logical task / route / agent node         |\n| `client_id`  | `str`              | Identifier of the application or deployment that generated traffic  |\n| `request`    | `dict`             | Exact [`openai.ChatCompletion.create`](https://platform.openai.com/docs/api-reference/chat/create) payload received by the model |\n| `response`   | `dict`             | Exact `ChatCompletion` response returned by the model               |",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The `request` uses the OpenAI `ChatCompletions` request format and contains the following attributes:\n\n- `model` includes the Model ID used to generate the response.\n- `messages` includes a `system` message as well as a `user` query.\n- `tools` includes a list of functions and parameters available to the LLM to choose from, as well as their parameters and descriptions.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "DATA_PATH = data_dir / \"aiva_primary_assistant_dataset.jsonl\"\n\n!head -n1 {DATA_PATH} | jq",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The data points generated by AI Virtual Assistant in response to user queries are considered **ground truth**. \n\nGround truth data points are used to **evaluate** and **customize** more efficient models that can perform similarly to the current model. This customization process is analogous to a student-teacher distillation setup, where synthetic data generated from the teacher model is used to fine-tune a student model.\n\nNext, we'll load the data into Elasticsearch using a helper method `load_data_to_elasticsearch`, making it accessible to the Data Flywheel service.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from src.scripts.load_test_data import load_data_to_elasticsearch\n\nload_data_to_elasticsearch(file_path=DATA_PATH)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n<a id=\"2\"></a>\n## Step 2: Create a Data Flywheel Workflow from MLRun",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now it's time to move to your Orchestrated environmentâ€”MLRun.\n\nTo get into the MLRun environment, please follow the instructions below:\n\n1. Go to your Brev Launchable and click on the **Access** tab.\n2. In the **Access** tab, click on the **Share A Service** button to create the following services:\n   1. **mlrun-jupyter**: Port `30040`. This is the JupyterLab environment where you can execute mlrun workflows.\n   2. **mlrun-ui**: Port `30060`. This is the MLRun UI where you can monitor and manage your project with all the runs, functions, and artifacts.\n   3. **nuclio-dashboard**: Port `30050`. This is the Nuclio dashboard where you can monitor and manage your realtime functions.\n3. After creating the services, open each service in a new tab and continue from **mlrun-jupyter**.\n4. In the **mlrun-jupyter** tab, open a new terminal and clone the `mlrun-data-flywheel`. You can use the following command:\n   ```bash\n   git clone https://github.com/mlrun/nvidia-data-flywheel.git\n   ```",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "![Services](./services.png)",
      "metadata": {}
    }
  ]
}