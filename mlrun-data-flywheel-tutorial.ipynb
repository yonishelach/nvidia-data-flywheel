{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ed5d53-95a6-4b9e-9313-25948f2c8392",
   "metadata": {},
   "source": [
    "# Data-FlyWheel Blueprint Orchestrated By MLRun\n",
    "\n",
    "This notebook demonstrates how MLRun orchestrates NVIDIA NIM microservices, providing automatic tracking, logging, and MLOps best practices. Each workflow step is modular - NIM microservices can work independently or as part of orchestrated workflows.\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "* **Infrastructure Abstraction**: MLRun eliminates glue code complexity, letting you focus on your use case rather than infrastructure management.\n",
    "* **Complete Lifecycle Management**: From development to production, MLRun handles resource management, auto-scaling, and real-time monitoring.\n",
    "* **Future Development**: Ongoing iterations will further reduce boilerplate code, making the transition from concept to production even more streamlined.\n",
    "\n",
    "To learn more about MLRun, visit [mlrun.org](https://mlrun.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dcfb1d-8411-43e4-a83c-82b82d99ef0b",
   "metadata": {},
   "source": [
    "## 1. Creating an MLRun project\n",
    "MLRun Project is a container for all your work on a particular ML application.\n",
    "Projects host functions, workflows, artifacts (datasets, models, etc.), features (sets, vectors), and configuration (parameters, secrets, source, etc.).\n",
    "\n",
    "For this project please set up the following variables:\n",
    "- `NGC_API_KEY` - Following the instructions at [Generating NGC API Keys](https://docs.nvidia.com/ngc/gpu-cloud/ngc-private-registry-user-guide/index.html#generating-api-key)\n",
    "- `docker_registry` - The Project image (docker image for the workflow's functions) that it will use. Notice that this image was prepared in the previous notebook and all the docker registry credentials are set."
   ]
  },
  {
   "cell_type": "code",
   "id": "41a300ed-7454-4090-92b6-097f3683f50d",
   "metadata": {},
   "source": [
    "import mlrun\n",
    "import os\n",
    "\n",
    "os.environ['NGC_API_KEY'] = '<your_ngc_api_key>'\n",
    "docker_registry = '<your_docker_registry_name>'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ff85f1ea-4060-4cd2-8753-243c15668f99",
   "metadata": {},
   "source": [
    "project = mlrun.get_or_create_project(\n",
    "    name=\"flywheel\",\n",
    "    parameters={\n",
    "        \"image\": f\"{docker_registry}/mlrun-data-flywheel:latest\",\n",
    "        \"source\": \"git://github.com/mlrun/nvidia-data-flywheel.git\",\n",
    "    },    \n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "27a1c74c-7f9e-4add-8173-ab82b955a7ca",
   "metadata": {},
   "source": [
    "Now the project is set with all you need to run the Data-FlyWheel workflow.\n",
    "\n",
    "It contains all the necessary functions and the workflow itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c38b3d-b7a5-4e96-bdcd-18c992f49137",
   "metadata": {},
   "source": [
    "## 2. Running Data-FlyWheel job with MLRun\n",
    "\n",
    "> **Notice**: For this initial version, the workflow can be run only with one configuration at a time.\n",
    "> To run multiple configurations, run each configuration separately.\n",
    "\n",
    "In the image below you can see an example of the view from the MLRun UI of the Data-Flywheel workflow in action. It can be found in project > Jobs and Workflows > Monitor Workflows.\n",
    "![Workflow UI](./img/workflow-ui.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "1cc489f7-dbde-4b97-95a0-2d1958f8e6c3",
   "metadata": {},
   "source": [
    "configs = [\n",
    "    {\n",
    "        \"model_name\": \"meta/llama-3.2-1b-instruct\",\n",
    "        \"context_length\": 8192,\n",
    "        \"gpus\": 1,\n",
    "        \"pvc_size\": \"25Gi\",\n",
    "        \"tag\": \"1.8.3\",\n",
    "        \"customization_enabled\": True\n",
    "    }\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "60296598-dd2f-4215-98fb-dcbf7e626ade",
   "metadata": {},
   "source": [
    "## 2.1. Initial Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6943913-5bd6-44f3-a392-f116fb516bf0",
   "metadata": {},
   "source": [
    "For this tutorial, we will target the primary customer service agent by setting the `workload_id` to \"primary_assistant\" and we will set `client_id` to \"aiva-1\" which has **300** data points."
   ]
  },
  {
   "cell_type": "code",
   "id": "b0371f22-2d36-4ef6-b17e-355d56e715d0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "data_fly_wheel_workflow = project.run(\n",
    "    name=\"data-flywheel-job\",\n",
    "    arguments={\n",
    "        \"workload_id\": \"primary_assistant\",\n",
    "        \"client_id\": \"aiva-1\",\n",
    "        \"configs\": configs,\n",
    "    },\n",
    "    watch=True,\n",
    "    engine=\"remote\",\n",
    "    dirty=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "49c970eb-603e-4ffb-8376-2eafc01766c2",
   "metadata": {},
   "source": [
    "## 2.2. Show Continuous Improvement (Optional)\n",
    "To extend the flywheel run with additional data, we’ll launch a new job using `client_id` set to \"aiva-2\", which includes **500** data points, to evaluate the impact of increased data volume on performance."
   ]
  },
  {
   "cell_type": "code",
   "id": "245a4b79-a96a-4724-abaa-e89784f7ffaa",
   "metadata": {},
   "source": [
    "data_fly_wheel_workflow = project.run(\n",
    "    name=\"data-flywheel-job\",\n",
    "    arguments={\n",
    "        \"workload_id\": \"primary_assistant\",\n",
    "        \"client_id\": \"aiva-2\",\n",
    "        \"configs\": configs,\n",
    "    },\n",
    "    watch=True,\n",
    "    engine=\"remote\",\n",
    "    dirty=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a911e7c5-b854-4d98-96d2-347127a6a855",
   "metadata": {},
   "source": [
    "Assuming we have now collected even more data points, let's kick off another flywheel run by setting `client_id` to \"aiva-3\" which includes **1,000** records."
   ]
  },
  {
   "cell_type": "code",
   "id": "055d998e-b812-4ab1-bf97-f037bc06f5a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "data_fly_wheel_workflow = project.run(\n",
    "    name=\"data-flywheel-job\",\n",
    "    arguments={\n",
    "        \"workload_id\": \"primary_assistant\",\n",
    "        \"client_id\": \"aiva-3\",\n",
    "        \"configs\": configs,\n",
    "    },\n",
    "    watch=True,\n",
    "    engine=\"remote\",\n",
    "    dirty=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2d05b1d8-d71b-4bd4-92be-d1bcddfd5c7c",
   "metadata": {},
   "source": [
    "After the run with 1,000 data points, we should observe the customized model’s score approaching 1.0. This indicates that the `LLama-3.2-1B-instruct` model achieves accuracy comparable to the much larger `LLama-3.3-70B-instruct` base model deployed in AI Virtual Assistant, while significantly reducing latency and compute usage thanks to its smaller size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun",
   "language": "python",
   "name": "mlrun"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
